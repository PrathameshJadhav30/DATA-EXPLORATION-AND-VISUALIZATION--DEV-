{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Necessary Libraries\n",
        "First, import the libraries required for web crawling.\n"
      ],
      "metadata": {
        "id": "kmeK0dmJi8Jd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VsW9Kewizcw",
        "outputId": "f8cd73c9-bb0f-4b2e-cd21-779a54b3ba55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2024.8.30)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, webdriver-manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install webdriver-manager\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define the Target Website and Send HTTP Request\n",
        "Specify the URL of the website you want to scrape and send an HTTP request to the server.\n"
      ],
      "metadata": {
        "id": "joLGfgjzjQfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://example.com'\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        " print(\"Successfully accessed the website\")\n",
        "else:\n",
        " print(\"Failed to access the website\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erZK0NnUjVeG",
        "outputId": "7543a0c8-0d64-4611-8051-6e2b90dc4587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully accessed the website\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Step 3: Parse the HTML Content\n",
        "Parse the HTML content of the web page using BeautifulSoup.\n"
      ],
      "metadata": {
        "id": "U3x4H99tjclj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "print(\"Title of the page:\", soup.title.string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdwQgJ0fjjbD",
        "outputId": "8156fff2-da93-4c36-81bf-53c668166a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title of the page: Example Domain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Extract Information\n",
        "Extract specific information from the web page. For example, extract all the headings.\n"
      ],
      "metadata": {
        "id": "XCSBg3M2joxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headings = soup.find_all('h1')\n",
        "for heading in headings:\n",
        " print(heading.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tgku-ASjtnc",
        "outputId": "7a8363f8-062c-458b-c215-0755c59eb103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Domain\n"
          ]
        }
      ]
    }
  ]
}